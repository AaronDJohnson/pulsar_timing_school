{
 "metadata": {
  "name": "",
  "signature": "sha256:622dd86bfc863ce27074e567f0ac7c63860c3739ed4b56d0ab064cf017c51057"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Markov Chain Monte-Carlo Exercises"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from PAL2 import bayesutils as bu\n",
      "import scipy.special as ss\n",
      "import acor\n",
      "import sys\n",
      "\n",
      "%matplotlib inline\n",
      "%config InlineBackend.figure_format = 'retina'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 1: Create a simple MCMC sampler.\n",
      "\n",
      "Remember the basic pseudo-code for an MCMC sampler is the following\n",
      "\n",
      "\n",
      "1. Initialize $x_0$\n",
      "2. for i in range(N):\n",
      "  * draw $u$ from uniform distribution $\\mathcal{U(0,1)}$\n",
      "  * Sample $x_*$ from proposal distribution $q(x_*|x_i)$\n",
      "  * if $u > \\min{(1, \\frac{p(x_*)q(x_i|x_*)}{p(x_i)q(x_*|x_i)}})$\n",
      "    * $x_{i+1}=x_*$\n",
      "  * else\n",
      "    * $x_{i+1} = x_i$\n",
      "    \n",
      "Your sampler should do the following:\n",
      "\n",
      "1. Read in the log-likelihood function, log-prior function, step size, and number of iterations as arguments. \n",
      "2. Use a basic gaussian jump proposal with a user defined step size\n",
      "3. Store an array of the chain (i.e., and N x npar array, where N is the number of iterations and npar is the number of parameters)\n",
      "4. Track the acceptance rate of the sampler in order to diagnose its performance.\n",
      "\n",
      "If you are comfortable with python classes, the following sampler class template may be useful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SimpleMCMC(object):\n",
      "    \n",
      "    def __init__(self, lnlikefn, lnpriorfn, sigmas):\n",
      "        \n",
      "        pass\n",
      "        \n",
      "    def sample(self, p0, N):\n",
      "        \n",
      "        pass\n",
      "            \n",
      "    \n",
      "    def jump(self, p0):\n",
      "        \n",
      "        return q, qxy\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 2: Run MCMC Sampler on Scaled/Shifted student-t distribution\n",
      "\n",
      "Here you will be running your awesome MCMC sampler to obtain samples from a 1-d Scaled/Shifted Student-t distribution\n",
      "\n",
      "$$ p(x|\\nu,\\sigma,\\mu) = \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2)\\sqrt{\\nu\\pi}\\sigma}\\bigg(1+\\frac{1}{\\nu}\\big(\\frac{x-\\mu}{\\sigma}\\big)^2\\bigg)^{-(\\nu+1)/2}$$\n",
      "\n",
      "As a note you can use the gamma function from scipy\n",
      "\n",
      "```python\n",
      "import scipy.special as ss\n",
      "help(ss.gamma)\n",
      "```\n",
      "\n",
      "1. Construct the likelihood function for this distribution. For your function to work with the sampler you will need to either hard code $\\nu$, $\\sigma$, and $\\mu$ in the likelihood or instead use a class which stores these values as attributes and then define a likelihood method that just reads in the parameter vector.\n",
      "2. For this problem we are just going to use a uniform prior so your ln-prior function can just return 0.\n",
      "3. To get a feel for how MCMCs are dependent on the jumps, run your sampler for 100000 iterations with 4 different jump sizes 0.01, 0.1, 2, and 100. Use the parameters $\\nu=3$, $\\sigma=1$, and $\\mu=1$ for the distribution.\n",
      "4. For each run take note of the acceptance fraction (remember for 1-d problems the optimal acceptance rate is 57%). \n",
      "5. Also for each run plot the chain trace (i.e. the MCMC samples vs. iteration number), a histogram of the samples with the actual pdf above overplotted, and the correlation function vs lag (use the ``acor`` package for this.)\n",
      "\n",
      "From this you should be able to answer the following questions:\n",
      "\n",
      "* Do we really want a high acceptance rate?\n",
      "* Which jump size produces the best mixing results?\n",
      "\n",
      "Again, the following class template may be useful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class StudentTDistribution(object):\n",
      "    \n",
      "    def __init__(self, nu, sigma, mu):\n",
      "        \n",
      "        self.nu = nu\n",
      "        self.sigma = sigma\n",
      "        self.mu = mu\n",
      "        \n",
      "    def lnlikefn(self, x):\n",
      "        \n",
      "      return lnlike\n",
      "    \n",
      "    def lnpriorfn(self, x):\n",
      "        \n",
      "        return 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Plot PDF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Small Step Size"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Medium Step Size"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Good Step Size"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Large Step Size"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 3: Inferring the parameters of the distribution\n",
      "\n",
      "Now that you have run and tested your MCMC on a simple 1-d problem of sampling a distribution, we will now pursue a problem more closely related to our pulsar timing analysis: Inferring the parameters of the distribution given some samples from that distribution. In the simplest case one could think of trying to estimate the variance of pulsar timing residuals given the residuals. Of course you would not need an MCMC for that but you could still use it.\n",
      "\n",
      "In this exercise we are going to use the samples that you created in the previous exercise as *data* for this problem.\n",
      "\n",
      "The likelihood function that you will use is \n",
      "\n",
      "$$ p(x|\\nu, \\sigma, \\mu) = \\prod_{i=1}^N \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2)\\sqrt{\\nu\\pi}\\sigma}\\bigg(1+\\frac{1}{\\nu}\\big(\\frac{x_i-\\mu}{\\sigma}\\big)^2\\bigg)^{-(\\nu+1)/2} $$\n",
      "\n",
      "Take the priors on the free parameters to be uniform in  $\\nu\\in[0.1, 10]$, $\\sigma\\in[0.1,10]$, $\\mu\\in[-5,5]$. This means that in your ln-prior function you should check to make sure that the free parameters lie within this range, else you should return ``-np.inf``.\n",
      "\n",
      "To summarize, you should create a function that reads in your free parameters (in vector form) and returns the natural logarithm of the above likelihood function. Further you should create a function that checks to make sure that your parameters are in the above prior range.\n",
      "\n",
      "You can then run your sampler to produce a triangle plot (1 and 2-d posteriors) of your three free parameters $\\nu$, $\\sigma$, and $\\mu$.\n",
      "\n",
      "Specifically, you should:\n",
      "\n",
      "1. Use the best mixing version of your part 1 exercise to produce samples to be used as data. Use $\\nu=2$, $\\sigma=1$, and $\\mu=1$ for the distribution parameters. Do this for two cases $N=100$, and $N=1000$, that is to say use 100 and 1000 samples from your chain as input to this run.\n",
      "2. For each case (i.e. 100 and 1000 data points, respectively) run your MCMC, again trying to get 25% acceptance rate by varying the jump sizes. \n",
      "3. For each case make a triangle plot. You can do this using the ``triplot`` function from ``bayesutils`` (i.e. ``from PAL2 import bayesutils``.)\n",
      "4. Also make a chain trace of each parameter to diagnose mixing.\n",
      "\n",
      "You should see that you can constrain the parameters of the PDF much better when you have more data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class StudentTLikelihood(object):\n",
      "    \n",
      "    def __init__(self, data):\n",
      "        \n",
      "        self.data = data\n",
      "        \n",
      "        # set minimum and maximum values\n",
      "        self.pmin = np.array([0.1, 0.1, -5])\n",
      "        self.pmax = np.array([10, 10, 5])\n",
      "        \n",
      "    def lnlikefn(self, x):\n",
      "        \n",
      "        return lnlike\n",
      "    \n",
      "    def lnpriorfn(self, x):\n",
      "        \n",
      "        if np.all(self.pmin < x) and np.all(self.pmax > x):\n",
      "            return 0\n",
      "        else:\n",
      "            return -np.inf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 100 samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1000 samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bonus Exercise 4: Multimodal distributions and Parallel (well not actually) tempering\n",
      "\n",
      "So far everything has been easy going. We have unimodal and mostly uncorellated parameters. However, in many situations we will see large (perhaps curving) degeneracies in parameters and possibly even multi-model behavior. As you will see, this poses a problem to traditional MCMC samplers in that they cannot explore the degeneracies efficiently (especially in large dimensions) and they are also very bad a moving between modes of a distribution. \n",
      "\n",
      "In this exercise we will attempt to sample from the distribution\n",
      "\n",
      "$$p(x,y)=\\frac{16}{3\\pi}\\bigg( e^{-x^2-(9+4x^2+8y)^2} + \\frac{1}{2}e^{-8x^2-8(y-2)^2}\\bigg)$$\n",
      "\n",
      "![alt text](figs/multimodal.pdf \"Multi-model distribution\")\n",
      "\n",
      "Obviously this distribution has two distinct modes.\n",
      "\n",
      "1. Try to write your own PTMCMC code.\n",
      "2. Run your sampler on the multimodal distribution\n",
      "3. Plot the triangle plot and feel good about yourself\n",
      "4. Run with 20 temperatures and compute the bayesian evidence via Thermodynamic integration."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Plot the PDF"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hint use meshgrid to make x and y coordinate arrays and use contour to plot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CurvedLikelihood(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        \n",
      "        self.pmin = np.array([-10, -10])\n",
      "        self.pmax = np.array([10, 10])\n",
      "    \n",
      "    def lnlikefn(self, x):\n",
      "        \n",
      "        return lnlike\n",
      "    \n",
      "    def lnpriorfn(self, x):\n",
      "        \n",
      "        if np.all(self.pmin < x) and np.all(self.pmax > x):\n",
      "            return 0\n",
      "        else:\n",
      "            return -np.inf               "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class PTMCMC(object):\n",
      "    \n",
      "    def __init__(self, lnlikefn, lnpriorfn, ndim, sigmas, ntemp):\n",
      "        \n",
      "        pass\n",
      "            \n",
      "    def PTlnlike(self, x):\n",
      "        \"\"\"\n",
      "        Function to return lnlike and lnprior in one go.\n",
      "        Might be useful if using map function\n",
      "        \"\"\"\n",
      "        return lnlike, lnprior\n",
      "    \n",
      "    def sample(self, p0, N):\n",
      "        \n",
      "        pass\n",
      "  \n",
      "    def jump(self, p0, temp=1):\n",
      " \n",
      "        return q, qxy           \n",
      "    \n",
      "    def temperature_swap(self, p, lnlike, lnprior):\n",
      " \n",
      "        return p, lnlike, lnprior"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}